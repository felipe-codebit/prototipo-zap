import { simpleNlpService } from './simple-nlp';
import { OpenAIService } from './openai';
import { ConversationContextManager } from './conversation-context';
import { ChatLogger } from './logger';
import { Intent, PlanoAulaData, PlanejamentoSemanalData } from '@/types';

export class MessageProcessor {
  static async processMessage(message: string, sessionId: string): Promise<string> {
    try {
      // Verifica√ß√£o priorit√°ria para comando "sair" - deve funcionar em qualquer momento
      const msg = message.toLowerCase().trim();
      if (['sair', 'cancelar', 'parar', 'reiniciar', 'recome√ßar', 'volta', 'voltar'].includes(msg) ||
          msg.includes('come√ßar de novo') || msg.includes('come√ßar denovo') ||
          msg.includes('sair daqui') || msg.includes('cancelar tudo')) {
        return this.handleSairIntent(sessionId);
      }

      const currentContext = ConversationContextManager.getContext(sessionId);
      const waitingFor = ConversationContextManager.getWaitingFor(sessionId);

      console.log('üöÄ [DEBUG] processMessage iniciado:', {
        message: message.substring(0, 50),
        sessionId: sessionId.substring(0, 8),
        waitingFor,
        currentIntent: currentContext.currentIntent,
        hasCollectedData: Object.keys(currentContext.collectedData).length > 0
      });

      // Se estamos esperando uma resposta espec√≠fica, verificar se usu√°rio quer cancelar primeiro
      if (waitingFor) {
        console.log('‚è≥ [DEBUG] Sistema est√° waitingFor:', waitingFor);

        // Verificar se o usu√°rio quer cancelar ou mudar de inten√ß√£o
        const intentAnalysis = await simpleNlpService.analyzeIntent(message, sessionId);
        console.log('üß† [DEBUG] An√°lise de inten√ß√£o durante waitingFor:', intentAnalysis);

        // Se detectou nega√ß√£o expl√≠cita, cancelar waitingFor
        if (message.toLowerCase().includes('n√£o quero') ||
            message.toLowerCase().includes('nao quero') ||
            message.toLowerCase().includes('cancela')) {

          console.log('üîÑ [DEBUG] Cancelando waitingFor - nega√ß√£o detectada');
          ConversationContextManager.clearWaitingFor(sessionId);
          ConversationContextManager.resetContextKeepingHistory(sessionId);
          console.log('üóëÔ∏è [DEBUG] Contexto resetado por nega√ß√£o');
          // Continuar com o processamento normal da nova inten√ß√£o
        }
        // Se detectou inten√ß√£o DIFERENTE da atual com alta confian√ßa, cancelar waitingFor
        else if (intentAnalysis.confidence > 0.7 &&
                 intentAnalysis.intent !== currentContext.currentIntent &&
                 intentAnalysis.intent !== 'continuar' &&
                 intentAnalysis.intent !== 'unclear') {

          console.log('üîÑ [DEBUG] Cancelando waitingFor - nova inten√ß√£o DIFERENTE detectada:', {
            nova: intentAnalysis.intent,
            atual: currentContext.currentIntent
          });
          ConversationContextManager.clearWaitingFor(sessionId);
          // Continuar com o processamento normal da nova inten√ß√£o
        } else {
          console.log('‚úÖ [DEBUG] Tentando processar como resposta espec√≠fica');
          // Tentar processar como resposta espec√≠fica
          const response = await this.processSpecificResponse(message, sessionId, waitingFor);
          if (response) {
            console.log('üéâ [DEBUG] Resposta espec√≠fica processada com sucesso!');
            return response;
          }

          console.log('ü§ñ [DEBUG] Falha no processamento tradicional, tentando LLM como fallback');
          // Se falhou, tentar com LLM baseado no contexto
          const llmResponse = await this.processResponseWithLLM(message, sessionId, waitingFor);
          if (llmResponse) {
            console.log('üéâ [DEBUG] LLM processou resposta com sucesso!');
            return llmResponse;
          }

          console.log('‚ùå [DEBUG] Falha ao processar resposta espec√≠fica, limpando waitingFor');
          // Se n√£o conseguiu processar como resposta espec√≠fica, limpar waitingFor
          ConversationContextManager.clearWaitingFor(sessionId);
        }
      }

      const currentIntent = currentContext.currentIntent;

      // Analisar inten√ß√£o
      const intentAnalysis = await simpleNlpService.analyzeIntent(message, sessionId);

      // Decidir qual inten√ß√£o usar: manter atual se estivermos coletando dados ou usar nova se clara
      let finalIntent = intentAnalysis.intent;
      let finalConfidence = intentAnalysis.confidence;

      // Se j√° temos uma inten√ß√£o ativa e estamos coletando dados, manter a inten√ß√£o atual
      // a menos que a nova inten√ß√£o seja muito clara (confian√ßa > 0.8)
      if (currentIntent &&
          currentIntent !== 'saudacao' &&
          currentIntent !== 'despedida' &&
          currentIntent !== 'unclear' &&
          Object.keys(currentContext.collectedData).length > 0) {

        // Se a nova inten√ß√£o n√£o √© muito clara, manter a atual
        if (intentAnalysis.confidence < 0.8 || intentAnalysis.intent === 'unclear') {
          finalIntent = currentIntent;
          finalConfidence = currentContext.intentConfidence;

          ChatLogger.logIntent(sessionId, `${finalIntent} (mantida)`, finalConfidence, message);
        }
      }

      // Atualizar contexto
      ConversationContextManager.updateIntent(sessionId, finalIntent, finalConfidence);

      // N√£o processar entidades automaticamente para evitar confus√£o
      // A extra√ß√£o agora √© feita apenas quando esperamos uma resposta espec√≠fica

      // Gerar resposta baseada na inten√ß√£o
      return await this.generateResponseByIntent(message, sessionId, finalIntent);

    } catch (error) {
      ChatLogger.logError(sessionId, error as Error, { message });
      return 'Desculpe, ocorreu um erro ao processar sua mensagem. Pode tentar novamente?';
    }
  }

  private static async processSpecificResponse(message: string, sessionId: string, waitingFor: string): Promise<string | null> {
    const currentContext = ConversationContextManager.getContext(sessionId);
    const currentIntent = currentContext.currentIntent;

    console.log('üîç [DEBUG] processSpecificResponse:', {
      message,
      waitingFor,
      currentIntent,
      collectedData: currentContext.collectedData,
      sessionId: sessionId.substring(0, 8)
    });

    if (!currentIntent) {
      console.log('‚ùå [DEBUG] Sem currentIntent, retornando null');
      return null;
    }

    switch (waitingFor) {
      case 'ano':
        console.log('üìö [DEBUG] Processando ano para plano_aula');
        if (currentIntent === 'plano_aula') {
          const anoProcessado = this.extractAnoEscolar(message);
          console.log('‚úÖ [DEBUG] Ano extra√≠do:', anoProcessado);

          ConversationContextManager.updateCollectedData(sessionId, 'ano', anoProcessado);
          ConversationContextManager.clearWaitingFor(sessionId);

          console.log('üéØ [DEBUG] Chamando handlePlanoAulaIntent ap√≥s coletar ano');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('‚ùå [DEBUG] currentIntent n√£o √© plano_aula:', currentIntent);
        }
        break;

      case 'tema':
        console.log('üìñ [DEBUG] Processando tema para plano_aula');
        if (currentIntent === 'plano_aula') {
          ConversationContextManager.updateCollectedData(sessionId, 'tema', message.trim());
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('üéØ [DEBUG] Chamando handlePlanoAulaIntent ap√≥s coletar tema');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('‚ùå [DEBUG] currentIntent n√£o √© plano_aula para tema:', currentIntent);
        }
        break;

      case 'dificuldade':
        console.log('‚öñÔ∏è [DEBUG] Processando dificuldade para plano_aula');
        if (currentIntent === 'plano_aula') {
          const msg = message.toLowerCase().trim();
          let difficulty = 'medio';

          if (msg.includes('f√°cil') || msg.includes('facil') || msg.includes('simples')) {
            difficulty = 'facil';
          } else if (msg.includes('dif√≠cil') || msg.includes('dificil') || msg.includes('avan√ßado')) {
            difficulty = 'dificil';
          }

          console.log('‚úÖ [DEBUG] Dificuldade processada:', difficulty);
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', difficulty);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('üéØ [DEBUG] Chamando handlePlanoAulaIntent ap√≥s coletar dificuldade');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('‚ùå [DEBUG] currentIntent n√£o √© plano_aula para dificuldade:', currentIntent);
        }
        break;

      case 'data_inicio':
        if (currentIntent === 'planejamento_semanal') {
          ConversationContextManager.updateCollectedData(sessionId, 'dataInicio', message.trim());
          ConversationContextManager.clearWaitingFor(sessionId);
          return await this.handlePlanejamentoSemanalIntent(sessionId, message);
        }
        break;
    }

    console.log('‚ùå [DEBUG] Nenhum case processado em processSpecificResponse');
    return null;
  }

  private static extractAnoEscolar(message: string): string {
    const msg = message.toLowerCase().trim();

    console.log('üî§ [DEBUG] Extraindo ano de:', msg);

    // Mapear varia√ß√µes comuns
    if (msg.includes('primeiro') || msg.includes('1¬∫') || msg === '1') return '1¬∫ ano';
    if (msg.includes('segundo') || msg.includes('2¬∫') || msg === '2') return '2¬∫ ano';
    if (msg.includes('terceiro') || msg.includes('3¬∫') || msg === '3') return '3¬∫ ano';
    if (msg.includes('quarto') || msg.includes('4¬∫') || msg === '4') return '4¬∫ ano';
    if (msg.includes('quinto') || msg.includes('5¬∫') || msg === '5') return '5¬∫ ano';
    if (msg.includes('sexto') || msg.includes('6¬∫') || msg === '6') return '6¬∫ ano';
    if (msg.includes('s√©timo') || msg.includes('7¬∫') || msg === '7') return '7¬∫ ano';
    if (msg.includes('oitavo') || msg.includes('8¬∫') || msg === '8') return '8¬∫ ano';
    if (msg.includes('nono') || msg.includes('9¬∫') || msg === '9') return '9¬∫ ano';
    if (msg.includes('m√©dio') || msg.includes('medio')) return 'Ensino M√©dio';

    // Se n√£o encontrou padr√£o, usar texto original
    console.log('‚ö†Ô∏è [DEBUG] N√£o encontrou padr√£o espec√≠fico, usando original');
    return message.trim();
  }

  private static async processResponseWithLLM(message: string, sessionId: string, waitingFor: string): Promise<string | null> {
    try {
      console.log('ü§ñ [DEBUG] Iniciando processamento LLM para waitingFor:', waitingFor);

      const context = ConversationContextManager.getContext(sessionId);
      const recentHistory = ConversationContextManager.getConversationHistory(sessionId).slice(-4);

      // Construir contexto para LLM
      let contextString = 'Hist√≥rico recente da conversa:\n';
      recentHistory.forEach(msg => {
        contextString += `${msg.sender === 'user' ? 'Professor' : 'Assistente'}: ${msg.text}\n`;
      });

      let prompt = '';

      switch (waitingFor) {
        case 'ano':
          prompt = `${contextString}

Analise a conversa acima. O assistente est√° perguntando sobre o ano escolar para criar um plano de aula.
A mensagem atual do professor √©: "${message}"

Identifique o ano escolar mencionado pelo professor. Responda APENAS com o ano no formato adequado (ex: "2¬∫ ano", "5¬∫ ano", "Ensino M√©dio").
Se n√£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'tema':
          prompt = `${contextString}

Analise a conversa acima. O assistente est√° perguntando sobre o tema ou habilidade BNCC para o plano de aula.
A mensagem atual do professor √©: "${message}"

Identifique o tema ou habilidade BNCC mencionado pelo professor. Responda APENAS com o tema/habilidade identificado.
Se n√£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'dificuldade':
          prompt = `${contextString}

Analise a conversa acima. O assistente est√° perguntando sobre o n√≠vel de dificuldade para o plano de aula.
A mensagem atual do professor √©: "${message}"

Identifique o n√≠vel de dificuldade mencionado. Responda APENAS com: "facil", "medio" ou "dificil".
Se n√£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'data_inicio':
          prompt = `${contextString}

Analise a conversa acima. O assistente est√° perguntando sobre a data de in√≠cio para o planejamento semanal.
A mensagem atual do professor √©: "${message}"

Identifique a data de in√≠cio mencionada pelo professor. Responda APENAS com a data identificada.
Se n√£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        default:
          console.log('‚ùå [DEBUG] waitingFor n√£o suportado pelo LLM:', waitingFor);
          return null;
      }

      const { OpenAIService } = await import('./openai');
      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'Voc√™ √© um assistente preciso que extrai informa√ß√µes espec√≠ficas de conversas.' },
          { role: 'user', content: prompt }
        ],
        max_tokens: 50,
        temperature: 0.1
      });

      const extractedValue = response.choices[0]?.message?.content?.trim();
      console.log('üß† [DEBUG] LLM extraiu valor:', extractedValue);

      if (!extractedValue || extractedValue === 'UNCLEAR') {
        console.log('‚ùå [DEBUG] LLM n√£o conseguiu extrair valor claro');
        return null;
      }

      // Processar valor extra√≠do
      switch (waitingFor) {
        case 'ano':
          ConversationContextManager.updateCollectedData(sessionId, 'ano', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('‚úÖ [DEBUG] LLM processou ano, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'tema':
          ConversationContextManager.updateCollectedData(sessionId, 'tema', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('‚úÖ [DEBUG] LLM processou tema, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'dificuldade':
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('‚úÖ [DEBUG] LLM processou dificuldade, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'data_inicio':
          ConversationContextManager.updateCollectedData(sessionId, 'dataInicio', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('‚úÖ [DEBUG] LLM processou data, chamando handlePlanejamentoSemanalIntent');
          return await this.handlePlanejamentoSemanalIntent(sessionId, message);
      }

      return null;

    } catch (error) {
      console.error('‚ùå [DEBUG] Erro no processamento LLM:', error);
      return null;
    }
  }

  private static processEntities(entities: Record<string, any>, sessionId: string, intent: Intent) {
    // Processar entidades espec√≠ficas para cada inten√ß√£o
    switch (intent) {
      case 'plano_aula':
        if (entities.ano) {
          ConversationContextManager.updateCollectedData(sessionId, 'ano', entities.ano);
        }
        if (entities.dificuldade) {
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', entities.dificuldade);
        }
        break;

      case 'calendario_escolar':
        // Aqui poder√≠amos processar entidades relacionadas a datas, per√≠odos, etc.
        break;
    }

    // Extrair informa√ß√µes adicionais da mensagem usando patterns
    this.extractAdditionalInfo(sessionId, intent, entities);
  }

  private static async extractAdditionalInfo(sessionId: string, intent: Intent, entities: Record<string, any>) {
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 3);
    const latestMessage = recentMessages[recentMessages.length - 1];
    const currentContext = ConversationContextManager.getContext(sessionId);

    if (!latestMessage) return;

    // Usar a inten√ß√£o atual se estivermos em coleta de dados
    const effectiveIntent = (currentContext.currentIntent &&
                           Object.keys(currentContext.collectedData).length > 0)
                           ? currentContext.currentIntent
                           : intent;

    // Usar LLM para extrair dados de forma inteligente
    if (effectiveIntent === 'plano_aula' || effectiveIntent === 'planejamento_semanal') {
      const extractedData = await OpenAIService.extractDataFromMessage(
        latestMessage,
        effectiveIntent,
        currentContext.collectedData,
        currentContext.waitingFor,
        sessionId
      );

      // Atualizar dados coletados com o que foi extra√≠do
      for (const [key, value] of Object.entries(extractedData)) {
        if (value) {
          ConversationContextManager.updateCollectedData(sessionId, key, value);
        }
      }
    }
  }

  // Fun√ß√µes antigas de extra√ß√£o baseadas em keywords foram removidas
  // Agora usamos extractDataFromMessage da OpenAIService que usa LLM

  /**
   * Infere o que o usu√°rio quer continuar com base no hist√≥rico da conversa
   */
  private static async inferContinuationIntent(
    conversationHistory: Array<{ sender: string; text: string }>,
    sessionId: string
  ): Promise<Intent | null> {
    try {
      const recentHistory = conversationHistory.slice(-6).map(msg =>
        `${msg.sender === 'user' ? 'Professor' : 'Ane'}: ${msg.text}`
      ).join('\n');

      const prompt = `Analise o hist√≥rico da conversa e identifique o que o professor quer continuar fazendo.

HIST√ìRICO:
${recentHistory}

O professor disse que quer "continuar". Com base no contexto, o que ele provavelmente quer fazer?

OP√á√ïES:
- plano_aula: Quer criar/continuar criando um plano de aula
- planejamento_semanal: Quer criar/continuar um planejamento semanal
- tira_duvidas: Quer fazer perguntas/tirar d√∫vidas
- null: N√£o h√° contexto claro do que continuar

Analise:
- O que a Ane sugeriu nas √∫ltimas mensagens?
- Qual era o t√≥pico da conversa?
- Houve algum plano/tarefa mencionado?

Retorne APENAS JSON: {"intent": "nome_ou_null", "confidence": 0.0}`;

      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'Voc√™ √© um analisador de contexto conversacional.' },
          { role: 'user', content: prompt }
        ],
        max_tokens: 100,
        temperature: 0.1
      });

      const result = response.choices[0]?.message?.content?.trim();
      if (!result) return null;

      const parsed = JSON.parse(result);
      if (parsed.confidence >= 0.7 && parsed.intent !== 'null') {
        return parsed.intent as Intent;
      }

      return null;
    } catch (error) {
      console.error('Erro ao inferir inten√ß√£o de continua√ß√£o:', error);
      return null;
    }
  }

  private static async generateResponseByIntent(message: string, sessionId: string, intent: Intent): Promise<string> {
    switch (intent) {
      case 'plano_aula':
        return this.handlePlanoAulaIntent(sessionId, message);

      case 'planejamento_semanal':
        return this.handlePlanejamentoSemanalIntent(sessionId, message);

      case 'tira_duvidas':
        return OpenAIService.generateResponse(message, sessionId);

      case 'saudacao':
        return await this.handleSaudacao(message, sessionId);

      case 'despedida':
        return this.handleDespedida(sessionId);

      case 'sair':
        return this.handleSairIntent(sessionId);

      case 'continuar':
        return this.handleContinuarIntent(sessionId, message);

      default:
        return this.handleUnclearIntent(message, sessionId);
    }
  }

  private static async handlePlanoAulaIntent(sessionId: string, message: string): Promise<string> {
    const missingData = ConversationContextManager.getMissingDataForPlanoAula(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar plano de aula
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanoAulaData;
      const planoAula = await OpenAIService.generatePlanoAula(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'plano_aula_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto ap√≥s gerar o plano
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planoAula}`;
    } else {
      // Ainda faltam dados, perguntar especificamente
      return await this.askForMissingPlanoAulaData(missingData, sessionId);
    }
  }

  private static async handlePlanejamentoSemanalIntent(sessionId: string, message: string): Promise<string> {
    const missingData = ConversationContextManager.getMissingDataForPlanejamentoSemanal(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar planejamento semanal
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanejamentoSemanalData;
      const planejamento = await OpenAIService.generatePlanejamentoSemanal(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'planejamento_semanal_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto ap√≥s gerar o planejamento
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planejamento}`;
    } else {
      // Ainda faltam dados
      return await this.askForMissingPlanejamentoSemanalData(missingData, sessionId);
    }
  }

  private static async askForMissingPlanoAulaData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    let missingField: string;
    let fieldKey: string;

    if (missingData.includes('ano')) {
      missingField = 'ano';
      fieldKey = 'ano';
    } else if (missingData.includes('tema ou habilidade BNCC')) {
      missingField = 'tema ou habilidade BNCC';
      fieldKey = 'tema';
    } else if (missingData.includes('n√≠vel de dificuldade')) {
      missingField = 'n√≠vel de dificuldade';
      fieldKey = 'dificuldade';
    } else {
      return 'üòä Estamos quase l√°! S√≥ preciso de mais algumas informa√ß√µes para criar um plano de aula perfeito para voc√™!';
    }

    // Gera a pergunta conversacional usando a LLM
    const question = await OpenAIService.generateConversationalQuestion(
      missingField,
      collectedData,
      conversationHistory,
      sessionId
    );

    ConversationContextManager.setWaitingFor(sessionId, fieldKey, question);
    return question;
  }

  private static async askForMissingPlanejamentoSemanalData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    let missingField: string;
    let fieldKey: string;

    if (missingData.includes('data de in√≠cio')) {
      missingField = 'data de in√≠cio';
      fieldKey = 'data_inicio';
    } else {
      // Fallback para outros campos faltantes
      return 'üéØ Quase l√°! S√≥ mais alguns detalhes e vamos criar um planejamento semanal incr√≠vel para voc√™!';
    }

    // Gera a pergunta conversacional usando a LLM
    const question = await OpenAIService.generateConversationalQuestion(
      missingField,
      collectedData,
      conversationHistory.map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.text
      })),
      sessionId
    );

    ConversationContextManager.setWaitingFor(sessionId, fieldKey, question);
    return question;
  }

  private static async handleSaudacao(message: string, sessionId: string): Promise<string> {
    console.log('üëã [DEBUG] Processando sauda√ß√£o com mensagem estruturada da ANE');

    // Verificar se √© uma sauda√ß√£o simples ou se tem solicita√ß√£o espec√≠fica
    const msg = message.toLowerCase().trim();
    const saudacoesSimples = ['oi', 'ol√°', 'ola', 'eae', 'oii', 'e a√≠'];
    const saudacoesComplementares = ['bom dia', 'boa tarde', 'boa noite', 'oi tudo bem', 'oi, tudo bem'];

    // Se for sauda√ß√£o simples, usar mensagem estruturada
    if (saudacoesSimples.includes(msg) ||
        saudacoesComplementares.some(saud => msg.includes(saud))) {

      console.log('‚úÖ [DEBUG] Sauda√ß√£o simples detectada, usando mensagem estruturada');

      return `Oi, eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí°
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? üòä`;
    }

    // Se a mensagem for mais complexa (ex: "oi, quero um plano de aula"), usar LLM
    console.log('ü§ñ [DEBUG] Sauda√ß√£o com solicita√ß√£o detectada, processando com LLM');

    try {
      const { OpenAIService } = await import('./openai');
      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const promptParaSaudacaoComSolicitacao = `
A mensagem do professor combina uma sauda√ß√£o com uma solicita√ß√£o espec√≠fica: ${message}
Reconhe√ßa o contexto da intera√ß√£o para decidir como prosseguir.

‚û°Ô∏è Regras de comportamento:

1. Sempre reconhe√ßa sauda√ß√µes e ‚Äúsmall talk‚Äù (ex.: ‚Äúoi, tudo bem?‚Äù, ‚Äúbom dia!‚Äù, ‚Äútudo certo?‚Äù) antes de qualquer instru√ß√£o, de forma natural e acolhedora.
2. Sua apresenta√ß√£o deve sempre usar como base a mensagem abaixo, adaptando a linguagem para soar natural e pr√≥xima do professor:
"Oi, eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí°  
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?"

3. Explique sempre o que voc√™ consegue fazer, mesmo quando houver uma solicita√ß√£o.
Liste claramente suas principais fun√ß√µes:
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento
4. Se o professor j√° trouxer uma solicita√ß√£o, adapte a explica√ß√£o acima ao contexto e incentive que ele d√™ mais detalhes.
5. Sempre finalize mostrando que √© um prazer ajudar.  

Assim, mesmo se o professor mandar apenas ‚ÄúOi, tudo bem?‚Äù, a resposta pode ser:

"Oi, tudo bem? Eu sou a ANE, sua assistente pedag√≥gica üë©üèΩ‚Äçüè´üí°.
Quero te mostrar rapidinho como posso te ajudar por aqui.
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Para come√ßar, me conta o ano e o tema ou habilidade que voc√™ est√° planejando?
Vai ser um prazer te ajudar!"

E se o professor mandar ‚ÄúOi, bom dia, me ajuda a planejar uma aula sobre fra√ß√µes para o 6¬∫ ano?‚Äù, a IA responde:

Oi, bom dia! Eu sou a ANE, sua assistente pedag√≥gica üë©üèΩ‚Äçüè´üí°.
Que √≥timo voc√™ j√° trazer seu pedido! Antes de come√ßarmos, deixa eu te contar rapidinho como posso te ajudar:
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Voc√™ mencionou fra√ß√µes para o 6¬∫ ano. Quer que eu sugira um planejamento completo com atividades ou prefere s√≥ ideias de metodologias para essa habilidade?`;

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: promptParaSaudacaoComSolicitacao },
          { role: 'user', content: message }
        ],
        max_tokens: 1500,
        temperature: 0.7
      });

      const botResponse = response.choices[0]?.message?.content ||
        `Oi, eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí°
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? üòä`;

      console.log('‚úÖ [DEBUG] Resposta LLM para sauda√ß√£o complexa gerada');
      return botResponse;

    } catch (error) {
      console.error('‚ùå [DEBUG] Erro no LLM para sauda√ß√£o:', error);
      // Fallback para mensagem estruturada
      return `Oi, eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí°
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? üòä`;
    }
  }

  private static async handleDespedida(sessionId: string): Promise<string> {
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'despedida',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.clearContext(sessionId);
    return response;
  }

  private static async handleSairIntent(sessionId: string): Promise<string> {
    // Registrar a mensagem do usu√°rio no hist√≥rico antes de resetar o contexto
    ConversationContextManager.addMessage(sessionId, {
      id: `user_${Date.now()}`,
      text: '[Usu√°rio solicitou reiniciar conversa]',
      sender: 'user',
      timestamp: new Date(),
      type: 'text'
    });

    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'reiniciar',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.resetContextKeepingHistory(sessionId);

    // Registrar a resposta do bot no hist√≥rico
    ConversationContextManager.addMessage(sessionId, {
      id: `bot_${Date.now()}`,
      text: response,
      sender: 'bot',
      timestamp: new Date(),
      type: 'text'
    });

    return response;
  }

  private static async handleContinuarIntent(sessionId: string, message: string): Promise<string> {
    const context = ConversationContextManager.getContext(sessionId);

    // Se j√° temos uma inten√ß√£o ativa, continuar com ela
    if (context.currentIntent && context.currentIntent !== 'saudacao' && context.currentIntent !== 'continuar') {
      return this.generateResponseByIntent(message, sessionId, context.currentIntent);
    }

    // Se n√£o temos inten√ß√£o ativa, analisar hist√≥rico para encontrar sugest√£o anterior
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 5);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Usar LLM para inferir o que o usu√°rio quer continuar baseado no contexto
    const inferredIntent = await this.inferContinuationIntent(conversationHistory, sessionId);

    if (inferredIntent) {
      ConversationContextManager.updateIntent(sessionId, inferredIntent, 0.9);
      return this.generateResponseByIntent(message, sessionId, inferredIntent);
    }

    // Se n√£o conseguiu identificar contexto, gerar resposta contextual
    return await OpenAIService.generateContextualResponse(
      'continuar_sem_contexto',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }

  private static async handleUnclearIntent(message: string, sessionId: string): Promise<string> {
    const msg = message.toLowerCase();
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Se o usu√°rio diz que n√£o quer algo ou est√° negando
    if (msg.includes('n√£o quero') || msg.includes('nao quero') ||
        msg.includes('n√£o preciso') || msg.includes('nao preciso')) {
      return await OpenAIService.generateContextualResponse(
        'negacao',
        {
          message,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );
    }

    // Verificar se parece uma pergunta (tira-d√∫vidas)
    if (msg.includes('?') || msg.includes('como') || msg.includes('que') ||
        msg.includes('qual') || msg.includes('quando') || msg.includes('onde') ||
        msg.includes('por que') || msg.includes('porque')) {

      // Processar como tira-d√∫vidas
      return await OpenAIService.generateResponse(message, sessionId);
    }

    // Fallback geral - inten√ß√£o n√£o clara
    return await OpenAIService.generateContextualResponse(
      'unclear_intent',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }
}