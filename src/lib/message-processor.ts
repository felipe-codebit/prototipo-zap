import { simpleNlpService } from './simple-nlp';
import { OpenAIService } from './openai';
import { ConversationContextManager } from './conversation-context';
import { ChatLogger } from './logger';
import { Intent, PlanoAulaData, PlanejamentoSemanalData } from '@/types';

export class MessageProcessor {
  static async processMessage(message: string, sessionId: string): Promise<string> {
    try {
      const msg = message.toLowerCase().trim();
      
      // Verifica√ß√£o priorit√°ria para comando "sair" - deve funcionar em qualquer momento
      if (['sair', 'cancelar', 'parar', 'reiniciar', 'recome√ßar', 'volta', 'voltar'].includes(msg) ||
          msg.includes('come√ßar de novo') || msg.includes('come√ßar denovo') ||
          msg.includes('sair daqui') || msg.includes('cancelar tudo')) {
        return this.handleSairIntent(sessionId);
      }

      // Verifica√ß√£o priorit√°ria para gera√ß√£o de PDF - DEVE vir antes da an√°lise de inten√ß√£o
      if (this.isPDFRequest(msg)) {
        console.log('üìÑ Solicita√ß√£o de PDF detectada:', message);
        console.log('üìÑ Interrompendo processamento normal para gerar PDF');
        return this.handlePDFRequest(sessionId, message);
      }

      const currentContext = ConversationContextManager.getContext(sessionId);
      console.log('üöÄ [DEBUG] processMessage iniciado:', {
        message: message.substring(0, 50),
        sessionId: sessionId.substring(0, 8),
        currentIntent: currentContext.currentIntent,
        hasCollectedData: Object.keys(currentContext.collectedData).length > 0
      });



      const currentIntent = currentContext.currentIntent;

      // Analisar inten√ß√£o
      const intentAnalysis = await simpleNlpService.analyzeIntent(message, sessionId);

      // Decidir qual inten√ß√£o usar: manter atual se estivermos coletando dados ou usar nova se clara
      let finalIntent = intentAnalysis.intent;
      let finalConfidence = intentAnalysis.confidence;

      // Se j√° temos uma inten√ß√£o ativa e estamos coletando dados, manter a inten√ß√£o atual
      // a menos que a nova inten√ß√£o seja muito clara (confian√ßa > 0.8)
      if (currentIntent &&
          currentIntent !== 'saudacao' &&
          currentIntent !== 'despedida' &&
          currentIntent !== 'unclear' &&
          Object.keys(currentContext.collectedData).length > 0) {

        // Se a nova inten√ß√£o n√£o √© muito clara, manter a atual
        if (intentAnalysis.confidence < 0.8 || intentAnalysis.intent === 'unclear') {
          finalIntent = currentIntent;
          finalConfidence = currentContext.intentConfidence;

          ChatLogger.logIntent(sessionId, `${finalIntent} (mantida)`, finalConfidence, message);
        }
      }

      // Atualizar contexto
      ConversationContextManager.updateIntent(sessionId, finalIntent, finalConfidence);



      // Gerar resposta baseada na inten√ß√£o
      return await this.generateResponseByIntent(message, sessionId, finalIntent);

    } catch (error) {
      ChatLogger.logError(sessionId, error as Error, { message });
      return 'Desculpe, ocorreu um erro ao processar sua mensagem. Pode tentar novamente?';
    }
  }





  private static async extractAdditionalInfo(sessionId: string, intent: Intent) {
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 3);
    const latestMessage = recentMessages[recentMessages.length - 1];
    const currentContext = ConversationContextManager.getContext(sessionId);

    if (!latestMessage) return;

    // Usar a inten√ß√£o atual se estivermos em coleta de dados
    const effectiveIntent = (currentContext.currentIntent &&
                           Object.keys(currentContext.collectedData).length > 0)
                           ? currentContext.currentIntent
                           : intent;

    // Usar LLM para extrair dados de forma inteligente
    if (effectiveIntent === 'plano_aula' || effectiveIntent === 'planejamento_semanal') {
      const extractedData = await OpenAIService.extractDataFromMessage(
        latestMessage,
        effectiveIntent,
        currentContext.collectedData,
        sessionId
      );

      // Atualizar dados coletados com o que foi extra√≠do
      for (const [key, value] of Object.entries(extractedData)) {
        if (value) {
          ConversationContextManager.updateCollectedData(sessionId, key, value);
        }
      }
    }
  }

  // Fun√ß√µes antigas de extra√ß√£o baseadas em keywords foram removidas
  // Agora usamos extractDataFromMessage da OpenAIService que usa LLM

  /**
   * Infere o que o usu√°rio quer continuar com base no hist√≥rico da conversa
   */
  private static async inferContinuationIntent(
    conversationHistory: Array<{ sender: string; text: string }>,
    sessionId: string
  ): Promise<Intent | null> {
    try {
      const recentHistory = conversationHistory.slice(-6).map(msg =>
        `${msg.sender === 'user' ? 'Professor' : 'Ane'}: ${msg.text}`
      ).join('\n');

      const prompt = `Analise o hist√≥rico da conversa e identifique o que o professor quer continuar fazendo.

HIST√ìRICO:
${recentHistory}

O professor disse que quer "continuar". Com base no contexto, o que ele provavelmente quer fazer?

OP√á√ïES:
- plano_aula: Quer criar/continuar criando um plano de aula
- planejamento_semanal: Quer criar/continuar um planejamento semanal
- tira_duvidas: Quer fazer perguntas/tirar d√∫vidas
- null: N√£o h√° contexto claro do que continuar

Analise:
- O que a Ane sugeriu nas √∫ltimas mensagens?
- Qual era o t√≥pico da conversa?
- Houve algum plano/tarefa mencionado?

Retorne APENAS JSON: {"intent": "nome_ou_null", "confidence": 0.0}`;

      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'Voc√™ √© um analisador de contexto conversacional.' },
          { role: 'user', content: prompt }
        ],
        max_tokens: 100,
        temperature: 0.1
      });

      const result = response.choices[0]?.message?.content?.trim();
      if (!result) return null;

      const parsed = JSON.parse(result);
      if (parsed.confidence >= 0.7 && parsed.intent !== 'null') {
        return parsed.intent as Intent;
      }

      return null;
    } catch (error) {
      console.error('Erro ao inferir inten√ß√£o de continua√ß√£o:', error);
      return null;
    }
  }

  private static async generateResponseByIntent(message: string, sessionId: string, intent: Intent): Promise<string> {
    switch (intent) {
      case 'plano_aula':
        return this.handlePlanoAulaIntent(sessionId);

      case 'planejamento_semanal':
        return this.handlePlanejamentoSemanalIntent(sessionId);

      case 'tira_duvidas':
        return OpenAIService.generateResponse(message, sessionId);

      case 'saudacao':
        return await this.handleSaudacao(message, sessionId);

      case 'despedida':
        return this.handleDespedida(sessionId);

      case 'sair':
        return this.handleSairIntent(sessionId);

      case 'continuar':
        return this.handleContinuarIntent(sessionId, message);

      default:
        return this.handleUnclearIntent(message, sessionId);
    }
  }

  private static async handlePlanoAulaIntent(sessionId: string): Promise<string> {
    // Extrair informa√ß√µes da mensagem atual no contexto da inten√ß√£o
    await this.extractAdditionalInfo(sessionId, 'plano_aula');

    const missingData = ConversationContextManager.getMissingDataForPlanoAula(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar plano de aula
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanoAulaData;
      const planoAula = await OpenAIService.generatePlanoAula(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'plano_aula_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto ap√≥s gerar o plano
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planoAula}`;
    } else {
      // Ainda faltam dados, perguntar especificamente
      return await this.askForMissingPlanoAulaData(missingData, sessionId);
    }
  }

  private static async handlePlanejamentoSemanalIntent(sessionId: string): Promise<string> {
    // Extrair informa√ß√µes da mensagem atual no contexto da inten√ß√£o
    await this.extractAdditionalInfo(sessionId, 'planejamento_semanal');

    const missingData = ConversationContextManager.getMissingDataForPlanejamentoSemanal(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar planejamento semanal
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanejamentoSemanalData;
      const planejamento = await OpenAIService.generatePlanejamentoSemanal(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'planejamento_semanal_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto ap√≥s gerar o planejamento
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planejamento}`;
    } else {
      // Ainda faltam dados
      return await this.askForMissingPlanejamentoSemanalData(missingData, sessionId);
    }
  }

  private static async askForMissingPlanoAulaData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Gera a pergunta conversacional usando a LLM para todos os dados faltantes
    const question = await OpenAIService.generateConversationalQuestion(
      missingData.join(', '), // Passa todos os campos faltantes
      collectedData,
      conversationHistory.map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.text
      })),
      sessionId
    );

    return question;
  }

  private static async askForMissingPlanejamentoSemanalData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Gera a pergunta conversacional usando a LLM para todos os dados faltantes
    const question = await OpenAIService.generateConversationalQuestion(
      missingData.join(', '), // Passa todos os campos faltantes
      collectedData,
      conversationHistory.map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.text
      })),
      sessionId
    );

    return question;
  }

  private static async handleSaudacao(message: string, sessionId: string): Promise<string> {
    console.log('üëã [DEBUG] Processando sauda√ß√£o com LLM');

    try {
      const { OpenAIService } = await import('./openai');
      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const promptParaSaudacao = `
A mensagem do professor: "${message}"

‚û°Ô∏è Regras de comportamento:

1. SEMPRE reconhe√ßa sauda√ß√µes e "small talk" (ex.: "oi, tudo bem?", "bom dia!", "tudo certo?", "como voc√™ pode ajudar?") antes de qualquer instru√ß√£o, de forma natural e acolhedora.

2. Sua apresenta√ß√£o deve sempre usar como base a mensagem abaixo, adaptando a linguagem para soar natural e pr√≥xima do professor:
"Oi, eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí°  
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?"

3. SEMPRE explique o que voc√™ consegue fazer, mesmo quando houver uma solicita√ß√£o espec√≠fica.
Liste claramente suas principais fun√ß√µes:
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

4. Se o professor j√° trouxer uma solicita√ß√£o, adapte a explica√ß√£o acima ao contexto e incentive que ele d√™ mais detalhes.

5. SEMPRE finalize mostrando que √© um prazer ajudar.

EXEMPLOS DE RESPOSTAS:

Se o professor mandar apenas "Oi, tudo bem?":
"Oi, tudo bem? Eu sou a ANE, sua assistente pedag√≥gica üë©üèΩ‚Äçüè´üí°.
Quero te mostrar rapidinho como posso te ajudar por aqui.
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Para come√ßar, me conta o ano e o tema ou habilidade que voc√™ est√° planejando?
Vai ser um prazer te ajudar!"

Se o professor mandar "Como voc√™ pode ajudar?" ou "O que voc√™ faz?":
"Oi! Eu sou a ANE, sua assistente pedag√≥gica üë©üèΩ‚Äçüè´üí°.
Que bom voc√™ perguntar! Vou te mostrar rapidinho como posso te ajudar por aqui.
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Para come√ßar, me conta o ano e o tema ou habilidade que voc√™ est√° planejando?
Vai ser um prazer te ajudar!"

Se o professor mandar "Oi, bom dia, me ajuda a planejar uma aula sobre fra√ß√µes para o 6¬∫ ano?":
"Oi, bom dia! Eu sou a ANE, sua assistente pedag√≥gica üë©üèΩ‚Äçüè´üí°.
Que √≥timo voc√™ j√° trazer seu pedido! Antes de come√ßarmos, deixa eu te contar rapidinho como posso te ajudar:
üëâüèΩ Crio planejamentos de aula
üëâüèΩ Trago ideias de metodologias e atividades
üëâüèΩ Ajudo na reflex√£o sobre suas pr√°ticas pedag√≥gicas
üí¨ Voc√™ mencionou fra√ß√µes para o 6¬∫ ano. Quer que eu sugira um planejamento completo com atividades ou prefere s√≥ ideias de metodologias para essa habilidade?"
`;

      const response = await client.chat.completions.create({
        model: 'gpt-4-turbo',
        messages: [
          { role: 'system', content: promptParaSaudacao },
          { role: 'user', content: message }
        ],
        max_tokens: 300,
        temperature: 0.7
      });

      const botResponse = response.choices[0]?.message?.content ||
        `Oi! Eu sou a ANE, sua assistente pedag√≥gica. Como posso te ajudar?`;

      console.log('‚úÖ [DEBUG] Resposta LLM para sauda√ß√£o gerada');
      return botResponse;

    } catch (error) {
      console.error('‚ùå [DEBUG] Erro no LLM para sauda√ß√£o:', error);
      // Fallback em caso de erro
      return `Oi! Eu sou a ANE, sua assistente pedag√≥gica. üë©üèΩ‚Äçüè´üí° Como posso te ajudar hoje?`;
    }
  }

  private static async handleDespedida(sessionId: string): Promise<string> {
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'despedida',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.clearContext(sessionId);
    return response;
  }

  private static async handleSairIntent(sessionId: string): Promise<string> {
    // Registrar a mensagem do usu√°rio no hist√≥rico antes de resetar o contexto
    ConversationContextManager.addMessage(sessionId, {
      id: `user_${Date.now()}`,
      text: '[Usu√°rio solicitou reiniciar conversa]',
      sender: 'user',
      timestamp: new Date(),
      type: 'text'
    });

    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'reiniciar',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.resetContextKeepingHistory(sessionId);

    // Registrar a resposta do bot no hist√≥rico
    ConversationContextManager.addMessage(sessionId, {
      id: `bot_${Date.now()}`,
      text: response,
      sender: 'bot',
      timestamp: new Date(),
      type: 'text'
    });

    return response;
  }

  /**
   * Verifica se a mensagem √© uma solicita√ß√£o de PDF
   */
  private static isPDFRequest(message: string): boolean {
    const msg = message.toLowerCase().trim();
    
    const pdfKeywords = [
      'gerar pdf',
      'gerar em pdf',
      'fazer pdf',
      'criar pdf',
      'baixar pdf',
      'exportar pdf',
      'pdf do plano',
      'plano em pdf',
      'baixar plano',
      'exportar plano',
      'compartilhar pdf',
      'enviar pdf',
      'quero pdf',
      'preciso pdf',
      'fazer download',
      'baixar arquivo',
      'exportar arquivo',
      'quero gerar o pdf',
      'quero gerar pdf',
      'gerar o pdf',
      'fazer o pdf',
      'criar o pdf',
      'baixar o pdf',
      'gere o pdf',
      'gera o pdf',
      'gere pdf',
      'gera pdf'
    ];

    // Verifica√ß√£o mais robusta - qualquer men√ß√£o a PDF deve ser tratada como solicita√ß√£o
    const hasPDFKeyword = pdfKeywords.some(keyword => msg.includes(keyword));
    const hasPDFWord = msg.includes('pdf');
    const hasDownloadIntent = msg.includes('baixar') || msg.includes('download') || msg.includes('exportar');
    const hasGenerateIntent = msg.includes('gerar') || msg.includes('fazer') || msg.includes('criar') || msg.includes('gere') || msg.includes('gera');
    
    // Se cont√©m PDF e alguma a√ß√£o de gera√ß√£o/download, √© solicita√ß√£o de PDF
    const isPDFRequest = hasPDFKeyword || (hasPDFWord && (hasDownloadIntent || hasGenerateIntent));
    
    console.log('üîç [DEBUG] Verifica√ß√£o de PDF:', {
      message: msg,
      hasPDFKeyword,
      hasPDFWord,
      hasDownloadIntent,
      hasGenerateIntent,
      isPDFRequest
    });
    
    return isPDFRequest;
  }

  /**
   * Processa solicita√ß√£o de PDF
   */
  private static async handlePDFRequest(sessionId: string, message: string): Promise<string> {
    try {
      console.log('üìÑ Processando solicita√ß√£o de PDF...');
      
      // Buscar o √∫ltimo plano de aula gerado no hist√≥rico
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const lastPlanoMessage = conversationHistory
        .filter(msg => msg.sender === 'bot' && 
                      (msg.text.includes('Prontinho! Aqui est√° o seu plano de aula') || 
                       msg.text.includes('### Plano de Aula:')))
        .pop();

      if (!lastPlanoMessage) {
        return 'N√£o encontrei um plano de aula recente para gerar o PDF. Voc√™ precisa gerar um plano de aula primeiro! üòä';
      }

      // Extrair o conte√∫do do plano (remover a parte de pr√≥ximos passos)
      const planoContent = this.extractPlanoContent(lastPlanoMessage.text);
      
      if (!planoContent) {
        return 'N√£o consegui extrair o conte√∫do do plano de aula. Tente gerar um novo plano! üòä';
      }

      // Gerar resposta informando que o PDF est√° sendo criado
      const response = `Perfeito! Vou gerar o PDF do seu plano de aula para voc√™! üìÑ‚ú®

O arquivo ser√° baixado automaticamente em alguns segundos.

Enquanto isso, posso te ajudar com:
üëâüèΩ Criar outro plano de aula
üëâüèΩ Ajustar este plano
üëâüèΩ Planejamento semanal
üëâüèΩ Tirar d√∫vidas pedag√≥gicas

O que voc√™ gostaria de fazer agora?`;

      // Armazenar o conte√∫do do plano para gera√ß√£o de PDF
      ConversationContextManager.updateCollectedData(sessionId, 'lastPlanoContent', planoContent);

      return response;

    } catch (error) {
      console.error('‚ùå Erro ao processar solicita√ß√£o de PDF:', error);
      ChatLogger.logError(sessionId, error as Error, { context: 'pdf_request' });
      return 'Desculpe, ocorreu um erro ao gerar o PDF. Tente novamente! üòä';
    }
  }

  /**
   * Extrai o conte√∫do do plano de aula da mensagem
   */
  private static extractPlanoContent(message: string): string | null {
    try {
      // Encontrar onde termina o plano e come√ßam os pr√≥ximos passos
      const nextStepsIndex = message.indexOf('Prontinho! Aqui est√° o seu plano de aula');
      
      if (nextStepsIndex === -1) {
        // Se n√£o encontrar a se√ß√£o de pr√≥ximos passos, retornar toda a mensagem
        return message;
      }

      // Retornar apenas o conte√∫do do plano (antes dos pr√≥ximos passos)
      return message.substring(0, nextStepsIndex).trim();
    } catch (error) {
      console.error('‚ùå Erro ao extrair conte√∫do do plano:', error);
      return null;
    }
  }


  private static async handleContinuarIntent(sessionId: string, message: string): Promise<string> {
    const context = ConversationContextManager.getContext(sessionId);

    // Se j√° temos uma inten√ß√£o ativa, continuar com ela
    if (context.currentIntent && context.currentIntent !== 'saudacao' && context.currentIntent !== 'continuar') {
      return this.generateResponseByIntent(message, sessionId, context.currentIntent);
    }

    // Se n√£o temos inten√ß√£o ativa, analisar hist√≥rico para encontrar sugest√£o anterior
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 5);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Usar LLM para inferir o que o usu√°rio quer continuar baseado no contexto
    const inferredIntent = await this.inferContinuationIntent(conversationHistory, sessionId);

    if (inferredIntent) {
      ConversationContextManager.updateIntent(sessionId, inferredIntent, 0.9);
      return this.generateResponseByIntent(message, sessionId, inferredIntent);
    }

    // Se n√£o conseguiu identificar contexto, gerar resposta contextual
    return await OpenAIService.generateContextualResponse(
      'continuar_sem_contexto',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }

  private static async handleUnclearIntent(message: string, sessionId: string): Promise<string> {
    const msg = message.toLowerCase();
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Se o usu√°rio diz que n√£o quer algo ou est√° negando
    if (msg.includes('n√£o quero') || msg.includes('nao quero') ||
        msg.includes('n√£o preciso') || msg.includes('nao preciso')) {
      return await OpenAIService.generateContextualResponse(
        'negacao',
        {
          message,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );
    }

    // Verificar se parece uma pergunta (tira-d√∫vidas)
    if (msg.includes('?') || msg.includes('como') || msg.includes('que') ||
        msg.includes('qual') || msg.includes('quando') || msg.includes('onde') ||
        msg.includes('por que') || msg.includes('porque')) {

      // Processar como tira-d√∫vidas
      return await OpenAIService.generateResponse(message, sessionId);
    }

    // Fallback geral - inten√ß√£o n√£o clara
    return await OpenAIService.generateContextualResponse(
      'unclear_intent',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }
}