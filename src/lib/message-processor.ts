import { simpleNlpService } from './simple-nlp';
import { OpenAIService } from './openai';
import { ConversationContextManager } from './conversation-context';
import { ChatLogger } from './logger';
import { Intent, PlanoAulaData, PlanejamentoSemanalData } from '@/types';

export class MessageProcessor {
  static async processMessage(message: string, sessionId: string): Promise<string> {
    try {
      // VerificaÃ§Ã£o prioritÃ¡ria para comando "sair" - deve funcionar em qualquer momento
      const msg = message.toLowerCase().trim();
      if (['sair', 'cancelar', 'parar', 'reiniciar', 'recomeÃ§ar', 'volta', 'voltar'].includes(msg) ||
          msg.includes('comeÃ§ar de novo') || msg.includes('comeÃ§ar denovo') ||
          msg.includes('sair daqui') || msg.includes('cancelar tudo')) {
        return this.handleSairIntent(sessionId);
      }

      const currentContext = ConversationContextManager.getContext(sessionId);
      const waitingFor = ConversationContextManager.getWaitingFor(sessionId);

      console.log('ğŸš€ [DEBUG] processMessage iniciado:', {
        message: message.substring(0, 50),
        sessionId: sessionId.substring(0, 8),
        waitingFor,
        currentIntent: currentContext.currentIntent,
        hasCollectedData: Object.keys(currentContext.collectedData).length > 0
      });

      // Se estamos esperando uma resposta especÃ­fica, verificar se usuÃ¡rio quer cancelar primeiro
      if (waitingFor) {
        console.log('â³ [DEBUG] Sistema estÃ¡ waitingFor:', waitingFor);

        // Verificar se o usuÃ¡rio quer cancelar ou mudar de intenÃ§Ã£o
        const intentAnalysis = await simpleNlpService.analyzeIntent(message, sessionId);
        console.log('ğŸ§  [DEBUG] AnÃ¡lise de intenÃ§Ã£o durante waitingFor:', intentAnalysis);

        // Se detectou negaÃ§Ã£o explÃ­cita, cancelar waitingFor
        if (message.toLowerCase().includes('nÃ£o quero') ||
            message.toLowerCase().includes('nao quero') ||
            message.toLowerCase().includes('cancela')) {

          console.log('ğŸ”„ [DEBUG] Cancelando waitingFor - negaÃ§Ã£o detectada');
          ConversationContextManager.clearWaitingFor(sessionId);
          ConversationContextManager.resetContextKeepingHistory(sessionId);
          console.log('ğŸ—‘ï¸ [DEBUG] Contexto resetado por negaÃ§Ã£o');
          // Continuar com o processamento normal da nova intenÃ§Ã£o
        }
        // Se detectou intenÃ§Ã£o DIFERENTE da atual com alta confianÃ§a, cancelar waitingFor
        else if (intentAnalysis.confidence > 0.7 &&
                 intentAnalysis.intent !== currentContext.currentIntent &&
                 intentAnalysis.intent !== 'continuar' &&
                 intentAnalysis.intent !== 'unclear') {

          console.log('ğŸ”„ [DEBUG] Cancelando waitingFor - nova intenÃ§Ã£o DIFERENTE detectada:', {
            nova: intentAnalysis.intent,
            atual: currentContext.currentIntent
          });
          ConversationContextManager.clearWaitingFor(sessionId);
          // Continuar com o processamento normal da nova intenÃ§Ã£o
        } else {
          console.log('âœ… [DEBUG] Tentando processar como resposta especÃ­fica');
          // Tentar processar como resposta especÃ­fica
          const response = await this.processSpecificResponse(message, sessionId, waitingFor);
          if (response) {
            console.log('ğŸ‰ [DEBUG] Resposta especÃ­fica processada com sucesso!');
            return response;
          }

          console.log('ğŸ¤– [DEBUG] Falha no processamento tradicional, tentando LLM como fallback');
          // Se falhou, tentar com LLM baseado no contexto
          const llmResponse = await this.processResponseWithLLM(message, sessionId, waitingFor);
          if (llmResponse) {
            console.log('ğŸ‰ [DEBUG] LLM processou resposta com sucesso!');
            return llmResponse;
          }

          console.log('âŒ [DEBUG] Falha ao processar resposta especÃ­fica, limpando waitingFor');
          // Se nÃ£o conseguiu processar como resposta especÃ­fica, limpar waitingFor
          ConversationContextManager.clearWaitingFor(sessionId);
        }
      }

      const currentIntent = currentContext.currentIntent;

      // Analisar intenÃ§Ã£o
      const intentAnalysis = await simpleNlpService.analyzeIntent(message, sessionId);

      // Decidir qual intenÃ§Ã£o usar: manter atual se estivermos coletando dados ou usar nova se clara
      let finalIntent = intentAnalysis.intent;
      let finalConfidence = intentAnalysis.confidence;

      // Se jÃ¡ temos uma intenÃ§Ã£o ativa e estamos coletando dados, manter a intenÃ§Ã£o atual
      // a menos que a nova intenÃ§Ã£o seja muito clara (confianÃ§a > 0.8)
      if (currentIntent &&
          currentIntent !== 'saudacao' &&
          currentIntent !== 'despedida' &&
          currentIntent !== 'unclear' &&
          Object.keys(currentContext.collectedData).length > 0) {

        // Se a nova intenÃ§Ã£o nÃ£o Ã© muito clara, manter a atual
        if (intentAnalysis.confidence < 0.8 || intentAnalysis.intent === 'unclear') {
          finalIntent = currentIntent;
          finalConfidence = currentContext.intentConfidence;

          ChatLogger.logIntent(sessionId, `${finalIntent} (mantida)`, finalConfidence, message);
        }
      }

      // Atualizar contexto
      ConversationContextManager.updateIntent(sessionId, finalIntent, finalConfidence);

      // NÃ£o processar entidades automaticamente para evitar confusÃ£o
      // A extraÃ§Ã£o agora Ã© feita apenas quando esperamos uma resposta especÃ­fica

      // Gerar resposta baseada na intenÃ§Ã£o
      return await this.generateResponseByIntent(message, sessionId, finalIntent);

    } catch (error) {
      ChatLogger.logError(sessionId, error as Error, { message });
      return 'Desculpe, ocorreu um erro ao processar sua mensagem. Pode tentar novamente?';
    }
  }

  private static async processSpecificResponse(message: string, sessionId: string, waitingFor: string): Promise<string | null> {
    const currentContext = ConversationContextManager.getContext(sessionId);
    const currentIntent = currentContext.currentIntent;

    console.log('ğŸ” [DEBUG] processSpecificResponse:', {
      message,
      waitingFor,
      currentIntent,
      collectedData: currentContext.collectedData,
      sessionId: sessionId.substring(0, 8)
    });

    if (!currentIntent) {
      console.log('âŒ [DEBUG] Sem currentIntent, retornando null');
      return null;
    }

    switch (waitingFor) {
      case 'ano':
        console.log('ğŸ“š [DEBUG] Processando ano para plano_aula');
        if (currentIntent === 'plano_aula') {
          const anoProcessado = this.extractAnoEscolar(message);
          console.log('âœ… [DEBUG] Ano extraÃ­do:', anoProcessado);

          ConversationContextManager.updateCollectedData(sessionId, 'ano', anoProcessado);
          ConversationContextManager.clearWaitingFor(sessionId);

          console.log('ğŸ¯ [DEBUG] Chamando handlePlanoAulaIntent apÃ³s coletar ano');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('âŒ [DEBUG] currentIntent nÃ£o Ã© plano_aula:', currentIntent);
        }
        break;

      case 'tema':
        console.log('ğŸ“– [DEBUG] Processando tema para plano_aula');
        if (currentIntent === 'plano_aula') {
          ConversationContextManager.updateCollectedData(sessionId, 'tema', message.trim());
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('ğŸ¯ [DEBUG] Chamando handlePlanoAulaIntent apÃ³s coletar tema');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('âŒ [DEBUG] currentIntent nÃ£o Ã© plano_aula para tema:', currentIntent);
        }
        break;

      case 'dificuldade':
        console.log('âš–ï¸ [DEBUG] Processando dificuldade para plano_aula');
        if (currentIntent === 'plano_aula') {
          const msg = message.toLowerCase().trim();
          let difficulty = 'medio';

          if (msg.includes('fÃ¡cil') || msg.includes('facil') || msg.includes('simples')) {
            difficulty = 'facil';
          } else if (msg.includes('difÃ­cil') || msg.includes('dificil') || msg.includes('avanÃ§ado')) {
            difficulty = 'dificil';
          }

          console.log('âœ… [DEBUG] Dificuldade processada:', difficulty);
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', difficulty);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('ğŸ¯ [DEBUG] Chamando handlePlanoAulaIntent apÃ³s coletar dificuldade');
          return await this.handlePlanoAulaIntent(sessionId, message);
        } else {
          console.log('âŒ [DEBUG] currentIntent nÃ£o Ã© plano_aula para dificuldade:', currentIntent);
        }
        break;

      case 'data_inicio':
        if (currentIntent === 'planejamento_semanal') {
          ConversationContextManager.updateCollectedData(sessionId, 'dataInicio', message.trim());
          ConversationContextManager.clearWaitingFor(sessionId);
          return await this.handlePlanejamentoSemanalIntent(sessionId, message);
        }
        break;
    }

    console.log('âŒ [DEBUG] Nenhum case processado em processSpecificResponse');
    return null;
  }

  private static extractAnoEscolar(message: string): string {
    const msg = message.toLowerCase().trim();

    console.log('ğŸ”¤ [DEBUG] Extraindo ano de:', msg);

    // Mapear variaÃ§Ãµes comuns
    if (msg.includes('primeiro') || msg.includes('1Âº') || msg === '1') return '1Âº ano';
    if (msg.includes('segundo') || msg.includes('2Âº') || msg === '2') return '2Âº ano';
    if (msg.includes('terceiro') || msg.includes('3Âº') || msg === '3') return '3Âº ano';
    if (msg.includes('quarto') || msg.includes('4Âº') || msg === '4') return '4Âº ano';
    if (msg.includes('quinto') || msg.includes('5Âº') || msg === '5') return '5Âº ano';
    if (msg.includes('sexto') || msg.includes('6Âº') || msg === '6') return '6Âº ano';
    if (msg.includes('sÃ©timo') || msg.includes('7Âº') || msg === '7') return '7Âº ano';
    if (msg.includes('oitavo') || msg.includes('8Âº') || msg === '8') return '8Âº ano';
    if (msg.includes('nono') || msg.includes('9Âº') || msg === '9') return '9Âº ano';
    if (msg.includes('mÃ©dio') || msg.includes('medio')) return 'Ensino MÃ©dio';

    // Se nÃ£o encontrou padrÃ£o, usar texto original
    console.log('âš ï¸ [DEBUG] NÃ£o encontrou padrÃ£o especÃ­fico, usando original');
    return message.trim();
  }

  private static async processResponseWithLLM(message: string, sessionId: string, waitingFor: string): Promise<string | null> {
    try {
      console.log('ğŸ¤– [DEBUG] Iniciando processamento LLM para waitingFor:', waitingFor);

      const context = ConversationContextManager.getContext(sessionId);
      const recentHistory = ConversationContextManager.getConversationHistory(sessionId).slice(-4);

      // Construir contexto para LLM
      let contextString = 'HistÃ³rico recente da conversa:\n';
      recentHistory.forEach(msg => {
        contextString += `${msg.sender === 'user' ? 'Professor' : 'Assistente'}: ${msg.text}\n`;
      });

      let prompt = '';

      switch (waitingFor) {
        case 'ano':
          prompt = `${contextString}

Analise a conversa acima. O assistente estÃ¡ perguntando sobre o ano escolar para criar um plano de aula.
A mensagem atual do professor Ã©: "${message}"

Identifique o ano escolar mencionado pelo professor. Responda APENAS com o ano no formato adequado (ex: "2Âº ano", "5Âº ano", "Ensino MÃ©dio").
Se nÃ£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'tema':
          prompt = `${contextString}

Analise a conversa acima. O assistente estÃ¡ perguntando sobre o tema ou habilidade BNCC para o plano de aula.
A mensagem atual do professor Ã©: "${message}"

Identifique o tema ou habilidade BNCC mencionado pelo professor. Responda APENAS com o tema/habilidade identificado.
Se nÃ£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'dificuldade':
          prompt = `${contextString}

Analise a conversa acima. O assistente estÃ¡ perguntando sobre o nÃ­vel de dificuldade para o plano de aula.
A mensagem atual do professor Ã©: "${message}"

Identifique o nÃ­vel de dificuldade mencionado. Responda APENAS com: "facil", "medio" ou "dificil".
Se nÃ£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        case 'data_inicio':
          prompt = `${contextString}

Analise a conversa acima. O assistente estÃ¡ perguntando sobre a data de inÃ­cio para o planejamento semanal.
A mensagem atual do professor Ã©: "${message}"

Identifique a data de inÃ­cio mencionada pelo professor. Responda APENAS com a data identificada.
Se nÃ£o conseguir identificar claramente, responda apenas "UNCLEAR".`;
          break;

        default:
          console.log('âŒ [DEBUG] waitingFor nÃ£o suportado pelo LLM:', waitingFor);
          return null;
      }

      const { OpenAIService } = await import('./openai');
      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'VocÃª Ã© um assistente preciso que extrai informaÃ§Ãµes especÃ­ficas de conversas.' },
          { role: 'user', content: prompt }
        ],
        max_tokens: 50,
        temperature: 0.1
      });

      const extractedValue = response.choices[0]?.message?.content?.trim();
      console.log('ğŸ§  [DEBUG] LLM extraiu valor:', extractedValue);

      if (!extractedValue || extractedValue === 'UNCLEAR') {
        console.log('âŒ [DEBUG] LLM nÃ£o conseguiu extrair valor claro');
        return null;
      }

      // Processar valor extraÃ­do
      switch (waitingFor) {
        case 'ano':
          ConversationContextManager.updateCollectedData(sessionId, 'ano', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('âœ… [DEBUG] LLM processou ano, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'tema':
          ConversationContextManager.updateCollectedData(sessionId, 'tema', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('âœ… [DEBUG] LLM processou tema, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'dificuldade':
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('âœ… [DEBUG] LLM processou dificuldade, chamando handlePlanoAulaIntent');
          return await this.handlePlanoAulaIntent(sessionId, message);

        case 'data_inicio':
          ConversationContextManager.updateCollectedData(sessionId, 'dataInicio', extractedValue);
          ConversationContextManager.clearWaitingFor(sessionId);
          console.log('âœ… [DEBUG] LLM processou data, chamando handlePlanejamentoSemanalIntent');
          return await this.handlePlanejamentoSemanalIntent(sessionId, message);
      }

      return null;

    } catch (error) {
      console.error('âŒ [DEBUG] Erro no processamento LLM:', error);
      return null;
    }
  }

  private static processEntities(entities: Record<string, any>, sessionId: string, intent: Intent) {
    // Processar entidades especÃ­ficas para cada intenÃ§Ã£o
    switch (intent) {
      case 'plano_aula':
        if (entities.ano) {
          ConversationContextManager.updateCollectedData(sessionId, 'ano', entities.ano);
        }
        if (entities.dificuldade) {
          ConversationContextManager.updateCollectedData(sessionId, 'nivelDificuldade', entities.dificuldade);
        }
        break;

      case 'calendario_escolar':
        // Aqui poderÃ­amos processar entidades relacionadas a datas, perÃ­odos, etc.
        break;
    }

    // Extrair informaÃ§Ãµes adicionais da mensagem usando patterns
    this.extractAdditionalInfo(sessionId, intent, entities);
  }

  private static async extractAdditionalInfo(sessionId: string, intent: Intent, entities: Record<string, any>) {
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 3);
    const latestMessage = recentMessages[recentMessages.length - 1];
    const currentContext = ConversationContextManager.getContext(sessionId);

    if (!latestMessage) return;

    // Usar a intenÃ§Ã£o atual se estivermos em coleta de dados
    const effectiveIntent = (currentContext.currentIntent &&
                           Object.keys(currentContext.collectedData).length > 0)
                           ? currentContext.currentIntent
                           : intent;

    // Usar LLM para extrair dados de forma inteligente
    if (effectiveIntent === 'plano_aula' || effectiveIntent === 'planejamento_semanal') {
      const extractedData = await OpenAIService.extractDataFromMessage(
        latestMessage,
        effectiveIntent,
        currentContext.collectedData,
        currentContext.waitingFor,
        sessionId
      );

      // Atualizar dados coletados com o que foi extraÃ­do
      for (const [key, value] of Object.entries(extractedData)) {
        if (value) {
          ConversationContextManager.updateCollectedData(sessionId, key, value);
        }
      }
    }
  }

  // FunÃ§Ãµes antigas de extraÃ§Ã£o baseadas em keywords foram removidas
  // Agora usamos extractDataFromMessage da OpenAIService que usa LLM

  /**
   * Infere o que o usuÃ¡rio quer continuar com base no histÃ³rico da conversa
   */
  private static async inferContinuationIntent(
    conversationHistory: Array<{ sender: string; text: string }>,
    sessionId: string
  ): Promise<Intent | null> {
    try {
      const recentHistory = conversationHistory.slice(-6).map(msg =>
        `${msg.sender === 'user' ? 'Professor' : 'Ane'}: ${msg.text}`
      ).join('\n');

      const prompt = `Analise o histÃ³rico da conversa e identifique o que o professor quer continuar fazendo.

HISTÃ“RICO:
${recentHistory}

O professor disse que quer "continuar". Com base no contexto, o que ele provavelmente quer fazer?

OPÃ‡Ã•ES:
- plano_aula: Quer criar/continuar criando um plano de aula
- planejamento_semanal: Quer criar/continuar um planejamento semanal
- tira_duvidas: Quer fazer perguntas/tirar dÃºvidas
- null: NÃ£o hÃ¡ contexto claro do que continuar

Analise:
- O que a Ane sugeriu nas Ãºltimas mensagens?
- Qual era o tÃ³pico da conversa?
- Houve algum plano/tarefa mencionado?

Retorne APENAS JSON: {"intent": "nome_ou_null", "confidence": 0.0}`;

      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'VocÃª Ã© um analisador de contexto conversacional.' },
          { role: 'user', content: prompt }
        ],
        max_tokens: 100,
        temperature: 0.1
      });

      const result = response.choices[0]?.message?.content?.trim();
      if (!result) return null;

      const parsed = JSON.parse(result);
      if (parsed.confidence >= 0.7 && parsed.intent !== 'null') {
        return parsed.intent as Intent;
      }

      return null;
    } catch (error) {
      console.error('Erro ao inferir intenÃ§Ã£o de continuaÃ§Ã£o:', error);
      return null;
    }
  }

  private static async generateResponseByIntent(message: string, sessionId: string, intent: Intent): Promise<string> {
    switch (intent) {
      case 'plano_aula':
        return this.handlePlanoAulaIntent(sessionId, message);

      case 'planejamento_semanal':
        return this.handlePlanejamentoSemanalIntent(sessionId, message);

      case 'tira_duvidas':
        return OpenAIService.generateResponse(message, sessionId);

      case 'saudacao':
        return await this.handleSaudacao(message, sessionId);

      case 'despedida':
        return this.handleDespedida(sessionId);

      case 'sair':
        return this.handleSairIntent(sessionId);

      case 'continuar':
        return this.handleContinuarIntent(sessionId, message);

      default:
        return this.handleUnclearIntent(message, sessionId);
    }
  }

  private static async handlePlanoAulaIntent(sessionId: string, message: string): Promise<string> {
    const missingData = ConversationContextManager.getMissingDataForPlanoAula(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar plano de aula
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanoAulaData;
      const planoAula = await OpenAIService.generatePlanoAula(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'plano_aula_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto apÃ³s gerar o plano
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planoAula}`;
    } else {
      // Ainda faltam dados, perguntar especificamente
      return await this.askForMissingPlanoAulaData(missingData, sessionId);
    }
  }

  private static async handlePlanejamentoSemanalIntent(sessionId: string, message: string): Promise<string> {
    const missingData = ConversationContextManager.getMissingDataForPlanejamentoSemanal(sessionId);

    if (missingData.length === 0) {
      // Todos os dados coletados, gerar planejamento semanal
      const data = ConversationContextManager.getCollectedData(sessionId) as PlanejamentoSemanalData;
      const planejamento = await OpenAIService.generatePlanejamentoSemanal(data, sessionId);

      // Gerar resposta contextual e conversacional
      const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);
      const contextualResponse = await OpenAIService.generateContextualResponse(
        'planejamento_semanal_completo',
        {
          collectedData: data,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );

      // IMPORTANTE: Limpar completamente o contexto apÃ³s gerar o planejamento
      ConversationContextManager.resetContextKeepingHistory(sessionId);

      return `${contextualResponse}\n\n${planejamento}`;
    } else {
      // Ainda faltam dados
      return await this.askForMissingPlanejamentoSemanalData(missingData, sessionId);
    }
  }

  private static async askForMissingPlanoAulaData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    let missingField: string;
    let fieldKey: string;

    if (missingData.includes('ano')) {
      missingField = 'ano';
      fieldKey = 'ano';
    } else if (missingData.includes('tema ou habilidade BNCC')) {
      missingField = 'tema ou habilidade BNCC';
      fieldKey = 'tema';
    } else if (missingData.includes('nÃ­vel de dificuldade')) {
      missingField = 'nÃ­vel de dificuldade';
      fieldKey = 'dificuldade';
    } else {
      return 'ğŸ˜Š Estamos quase lÃ¡! SÃ³ preciso de mais algumas informaÃ§Ãµes para criar um plano de aula perfeito para vocÃª!';
    }

    // Gera a pergunta conversacional usando a LLM
    const question = await OpenAIService.generateConversationalQuestion(
      missingField,
      collectedData,
      conversationHistory,
      sessionId
    );

    ConversationContextManager.setWaitingFor(sessionId, fieldKey, question);
    return question;
  }

  private static async askForMissingPlanejamentoSemanalData(missingData: string[], sessionId: string): Promise<string> {
    const collectedData = ConversationContextManager.getCollectedData(sessionId);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    let missingField: string;
    let fieldKey: string;

    if (missingData.includes('data de inÃ­cio')) {
      missingField = 'data de inÃ­cio';
      fieldKey = 'data_inicio';
    } else {
      // Fallback para outros campos faltantes
      return 'ğŸ¯ Quase lÃ¡! SÃ³ mais alguns detalhes e vamos criar um planejamento semanal incrÃ­vel para vocÃª!';
    }

    // Gera a pergunta conversacional usando a LLM
    const question = await OpenAIService.generateConversationalQuestion(
      missingField,
      collectedData,
      conversationHistory.map(msg => ({
        role: msg.sender === 'user' ? 'user' : 'assistant',
        content: msg.text
      })),
      sessionId
    );

    ConversationContextManager.setWaitingFor(sessionId, fieldKey, question);
    return question;
  }

  private static async handleSaudacao(message: string, sessionId: string): Promise<string> {
    console.log('ğŸ‘‹ [DEBUG] Processando saudaÃ§Ã£o com mensagem estruturada da ANE');

    // Verificar se Ã© uma saudaÃ§Ã£o simples ou se tem solicitaÃ§Ã£o especÃ­fica
    const msg = message.toLowerCase().trim();
    const saudacoesSimples = ['oi', 'olÃ¡', 'ola', 'eae', 'oii', 'e aÃ­'];
    const saudacoesComplementares = ['bom dia', 'boa tarde', 'boa noite', 'oi tudo bem', 'oi, tudo bem'];

    // Se for saudaÃ§Ã£o simples, usar mensagem estruturada
    if (saudacoesSimples.includes(msg) ||
        saudacoesComplementares.some(saud => msg.includes(saud))) {

      console.log('âœ… [DEBUG] SaudaÃ§Ã£o simples detectada, usando mensagem estruturada');

      return `Oi, eu sou a ANE, sua assistente pedagÃ³gica. ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? ğŸ˜Š`;
    }

    // Se a mensagem for mais complexa (ex: "oi, quero um plano de aula"), usar LLM
    console.log('ğŸ¤– [DEBUG] SaudaÃ§Ã£o com solicitaÃ§Ã£o detectada, processando com LLM');

    try {
      const { OpenAIService } = await import('./openai');
      const openai = await import('openai');
      const client = new openai.default({
        apiKey: process.env.OPENAI_API_KEY,
      });

      const promptParaSaudacaoComSolicitacao = `
A mensagem do professor combina uma saudaÃ§Ã£o com uma solicitaÃ§Ã£o especÃ­fica: ${message}
ReconheÃ§a o contexto da interaÃ§Ã£o para decidir como prosseguir.

â¡ï¸ Regras de comportamento:

1. Sempre reconheÃ§a saudaÃ§Ãµes e â€œsmall talkâ€ (ex.: â€œoi, tudo bem?â€, â€œbom dia!â€, â€œtudo certo?â€) antes de qualquer instruÃ§Ã£o, de forma natural e acolhedora.
2. Sua apresentaÃ§Ã£o deve sempre usar como base a mensagem abaixo, adaptando a linguagem para soar natural e prÃ³xima do professor:
"Oi, eu sou a ANE, sua assistente pedagÃ³gica. ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡  
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?"

3. Explique sempre o que vocÃª consegue fazer, mesmo quando houver uma solicitaÃ§Ã£o.
Liste claramente suas principais funÃ§Ãµes:
ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬ Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento
4. Se o professor jÃ¡ trouxer uma solicitaÃ§Ã£o, adapte a explicaÃ§Ã£o acima ao contexto e incentive que ele dÃª mais detalhes.
5. Sempre finalize mostrando que Ã© um prazer ajudar.  

Assim, mesmo se o professor mandar apenas â€œOi, tudo bem?â€, a resposta pode ser:

"Oi, tudo bem? Eu sou a ANE, sua assistente pedagÃ³gica ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡.
Quero te mostrar rapidinho como posso te ajudar por aqui.
ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬ Para comeÃ§ar, me conta o ano e o tema ou habilidade que vocÃª estÃ¡ planejando?
Vai ser um prazer te ajudar!"

E se o professor mandar â€œOi, bom dia, me ajuda a planejar uma aula sobre fraÃ§Ãµes para o 6Âº ano?â€, a IA responde:

Oi, bom dia! Eu sou a ANE, sua assistente pedagÃ³gica ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡.
Que Ã³timo vocÃª jÃ¡ trazer seu pedido! Antes de comeÃ§armos, deixa eu te contar rapidinho como posso te ajudar:
ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬ VocÃª mencionou fraÃ§Ãµes para o 6Âº ano. Quer que eu sugira um planejamento completo com atividades ou prefere sÃ³ ideias de metodologias para essa habilidade?`;

      const response = await client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: promptParaSaudacaoComSolicitacao },
          { role: 'user', content: message }
        ],
        max_tokens: 1500,
        temperature: 0.7
      });

      const botResponse = response.choices[0]?.message?.content ||
        `Oi, eu sou a ANE, sua assistente pedagÃ³gica. ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? ğŸ˜Š`;

      console.log('âœ… [DEBUG] Resposta LLM para saudaÃ§Ã£o complexa gerada');
      return botResponse;

    } catch (error) {
      console.error('âŒ [DEBUG] Erro no LLM para saudaÃ§Ã£o:', error);
      // Fallback para mensagem estruturada
      return `Oi, eu sou a ANE, sua assistente pedagÃ³gica. ğŸ‘©ğŸ½â€ğŸ«ğŸ’¡
Quero te mostrar rapidinho como posso te ajudar por aqui, tudo bem?

ğŸ‘‰ğŸ½ Crio planejamentos de aula
ğŸ‘‰ğŸ½ Trago ideias de metodologias e atividades
ğŸ‘‰ğŸ½ Ajudo na reflexÃ£o sobre suas prÃ¡ticas pedagÃ³gicas
ğŸ’¬Para te ajudar preciso saber o ano e tema ou habilidade do seu planejamento

Conte pra mim, como posso te ajudar hoje? ğŸ˜Š`;
    }
  }

  private static async handleDespedida(sessionId: string): Promise<string> {
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'despedida',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.clearContext(sessionId);
    return response;
  }

  private static async handleSairIntent(sessionId: string): Promise<string> {
    // Registrar a mensagem do usuÃ¡rio no histÃ³rico antes de resetar o contexto
    ConversationContextManager.addMessage(sessionId, {
      id: `user_${Date.now()}`,
      text: '[UsuÃ¡rio solicitou reiniciar conversa]',
      sender: 'user',
      timestamp: new Date(),
      type: 'text'
    });

    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    const response = await OpenAIService.generateContextualResponse(
      'reiniciar',
      {
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );

    ConversationContextManager.resetContextKeepingHistory(sessionId);

    // Registrar a resposta do bot no histÃ³rico
    ConversationContextManager.addMessage(sessionId, {
      id: `bot_${Date.now()}`,
      text: response,
      sender: 'bot',
      timestamp: new Date(),
      type: 'text'
    });

    return response;
  }

  private static async handleContinuarIntent(sessionId: string, message: string): Promise<string> {
    const context = ConversationContextManager.getContext(sessionId);

    // Se jÃ¡ temos uma intenÃ§Ã£o ativa, continuar com ela
    if (context.currentIntent && context.currentIntent !== 'saudacao' && context.currentIntent !== 'continuar') {
      return this.generateResponseByIntent(message, sessionId, context.currentIntent);
    }

    // Se nÃ£o temos intenÃ§Ã£o ativa, analisar histÃ³rico para encontrar sugestÃ£o anterior
    const recentMessages = ConversationContextManager.getRecentUserMessages(sessionId, 5);
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Usar LLM para inferir o que o usuÃ¡rio quer continuar baseado no contexto
    const inferredIntent = await this.inferContinuationIntent(conversationHistory, sessionId);

    if (inferredIntent) {
      ConversationContextManager.updateIntent(sessionId, inferredIntent, 0.9);
      return this.generateResponseByIntent(message, sessionId, inferredIntent);
    }

    // Se nÃ£o conseguiu identificar contexto, gerar resposta contextual
    return await OpenAIService.generateContextualResponse(
      'continuar_sem_contexto',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }

  private static async handleUnclearIntent(message: string, sessionId: string): Promise<string> {
    const msg = message.toLowerCase();
    const conversationHistory = ConversationContextManager.getConversationHistory(sessionId);

    // Se o usuÃ¡rio diz que nÃ£o quer algo ou estÃ¡ negando
    if (msg.includes('nÃ£o quero') || msg.includes('nao quero') ||
        msg.includes('nÃ£o preciso') || msg.includes('nao preciso')) {
      return await OpenAIService.generateContextualResponse(
        'negacao',
        {
          message,
          conversationHistory: conversationHistory.map(msg => ({
            role: msg.sender === 'user' ? 'user' : 'assistant',
            content: msg.text
          }))
        },
        sessionId
      );
    }

    // Verificar se parece uma pergunta (tira-dÃºvidas)
    if (msg.includes('?') || msg.includes('como') || msg.includes('que') ||
        msg.includes('qual') || msg.includes('quando') || msg.includes('onde') ||
        msg.includes('por que') || msg.includes('porque')) {

      // Processar como tira-dÃºvidas
      return await OpenAIService.generateResponse(message, sessionId);
    }

    // Fallback geral - intenÃ§Ã£o nÃ£o clara
    return await OpenAIService.generateContextualResponse(
      'unclear_intent',
      {
        message,
        conversationHistory: conversationHistory.map(msg => ({
          role: msg.sender === 'user' ? 'user' : 'assistant',
          content: msg.text
        }))
      },
      sessionId
    );
  }
}